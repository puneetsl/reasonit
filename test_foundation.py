#!/usr/bin/env python3
"""
Comprehensive test suite for ReasonIt foundation components.

This test script validates all core functionality without requiring API keys,
using mocks and simulated responses where necessary.
"""

import asyncio
import logging
import sys
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


async def test_models_and_types():
    """Test core data models and type definitions."""
    print("\nüß™ Testing Models and Types...")

    try:
        from models import (
            ContextVariant,
            MemoryEntry,
            OutcomeType,
            ReasoningRequest,
            ReasoningStrategy,
            ToolResult,
            ToolType,
        )

        # Test ReasoningRequest creation
        request = ReasoningRequest(
            query="Test query",
            strategy=ReasoningStrategy.CHAIN_OF_THOUGHT,
            context_variant=ContextVariant.STANDARD,
            confidence_threshold=0.8
        )
        assert request.query == "Test query"
        assert request.strategy == ReasoningStrategy.CHAIN_OF_THOUGHT
        print("  ‚úÖ ReasoningRequest model working")

        # Test ToolResult creation
        tool_result = ToolResult(
            tool_name="test_tool",
            tool_type=ToolType.CALCULATOR,
            input_data={"expression": "2+2"},
            output_data={"result": 4},
            success=True,
            execution_time=0.1
        )
        assert tool_result.success is True
        assert tool_result.output_data["result"] == 4
        print("  ‚úÖ ToolResult model working")

        # Test MemoryEntry creation
        memory_entry = MemoryEntry(
            query="Test memory query",
            strategy=ReasoningStrategy.REFLEXION,
            outcome=OutcomeType.SUCCESS,
            confidence_achieved=0.8,
            cost_incurred=0.05,
            time_taken=2.5,
            reflection="Test reflection",
            lessons=["Test lesson 1", "Test lesson 2"],
            context_used=ContextVariant.STANDARD
        )
        assert len(memory_entry.lessons) == 2
        print("  ‚úÖ MemoryEntry model working")

        print("  ‚úÖ All models and types working correctly")

    except Exception as e:
        print(f"  ‚ùå Models test failed: {e}")
        return False

    return True


async def test_context_generation():
    """Test context generation system."""
    print("\nüß™ Testing Context Generation...")

    try:
        from context import ContextGenerator, ContextVariant
        from models import ReasoningStrategy

        generator = ContextGenerator()

        # Test minified context
        original_prompt = "Solve the equation 2x + 5 = 13 for x"
        minified = await generator.generate_context(
            original_prompt,
            ContextVariant.MINIFIED,
            ReasoningStrategy.CHAIN_OF_THOUGHT
        )
        print(f"  üîç Minified: {len(original_prompt)} -> {len(minified)} chars")
        # For very short prompts, minified might be longer due to template overhead
        # but should be shorter than enriched
        assert len(minified) < len(original_prompt) * 2.0  # Reasonable upper bound
        print("  ‚úÖ Minified context generation working")

        # Test enriched context
        enriched = await generator.generate_context(
            original_prompt,
            ContextVariant.ENRICHED,
            ReasoningStrategy.CHAIN_OF_THOUGHT
        )
        print(f"  üîç Enriched: {len(original_prompt)} -> {len(enriched)} chars")
        assert len(enriched) > len(original_prompt) * 2  # Should be much longer
        assert "INSTRUCTIONS:" in enriched
        print("  ‚úÖ Enriched context generation working")

        # Test symbolic context
        symbolic = await generator.generate_context(
            original_prompt,
            ContextVariant.SYMBOLIC,
            ReasoningStrategy.CHAIN_OF_THOUGHT
        )
        print(f"  üîç Symbolic: {len(original_prompt)} -> {len(symbolic)} chars")
        print(f"  üîç Symbolic content preview: {symbolic[:100]}...")
        assert "SYMBOLIC" in symbolic or "equation" in symbolic.lower()
        print("  ‚úÖ Symbolic context generation working")

        # Test cost estimation
        cost_impact = generator.estimate_cost_impact(original_prompt, ContextVariant.ENRICHED)
        assert cost_impact["token_multiplier"] > 1.0
        assert "estimated_tokens" in cost_impact
        print("  ‚úÖ Cost estimation working")

        print("  ‚úÖ All context generation working correctly")

    except Exception as e:
        print(f"  ‚ùå Context generation test failed: {e}")
        return False

    return True


async def test_tool_framework():
    """Test tool integration framework."""
    print("\nüß™ Testing Tool Framework...")

    try:
        from tools import get_tool, list_available_tools

        # Test tool registration
        available_tools = list_available_tools()
        print(f"  üìã Available tools: {available_tools}")

        # Test getting specific tools
        python_tool = get_tool("execute_python")
        if python_tool:
            print("  ‚úÖ Python executor tool registered")

        search_tool = get_tool("search_web")
        if search_tool:
            print("  ‚úÖ Search tool registered")

        calc_tool = get_tool("calculate_expression")
        if calc_tool:
            print("  ‚úÖ Calculator tool registered")

        verify_tool = get_tool("verify_answer")
        if verify_tool:
            print("  ‚úÖ Verifier tool registered")

        assert len(available_tools) > 0
        print("  ‚úÖ Tool framework working correctly")

    except Exception as e:
        print(f"  ‚ùå Tool framework test failed: {e}")
        return False

    return True


async def test_calculator_tool():
    """Test calculator tool functionality."""
    print("\nüß™ Testing Calculator Tool...")

    try:
        from tools.calculator import CalculatorTool, SafeMathEvaluator

        # Test safe math evaluator
        evaluator = SafeMathEvaluator()

        # Basic arithmetic
        result = evaluator.evaluate("2 + 2 * 3")
        assert result == 8
        print("  ‚úÖ Basic arithmetic working")

        # Mathematical functions
        result = evaluator.evaluate("sqrt(16)")
        assert result == 4.0
        print("  ‚úÖ Math functions working")

        # Constants
        result = evaluator.evaluate("pi")
        assert abs(result - 3.14159) < 0.001
        print("  ‚úÖ Mathematical constants working")

        # Test calculator tool
        calc_tool = CalculatorTool()
        tool_result = await calc_tool.execute(expression="2 + 2")
        assert tool_result.success
        assert tool_result.output_data["result"] == 4
        print("  ‚úÖ Calculator tool execution working")

        print("  ‚úÖ Calculator tool working correctly")

    except Exception as e:
        print(f"  ‚ùå Calculator tool test failed: {e}")
        return False

    return True


async def test_python_executor():
    """Test Python executor tool."""
    print("\nüß™ Testing Python Executor...")

    try:
        from tools.python_executor import PythonExecutorTool, PythonSandbox

        # Test sandbox validation
        sandbox = PythonSandbox()

        # Test safe code
        try:
            sandbox.validate_code("print(2 + 2)")
            print("  ‚úÖ Safe code validation working")
        except Exception:
            print("  ‚ùå Safe code validation failed")
            return False

        # Test unsafe code detection
        try:
            sandbox.validate_code("import os; os.system('rm -rf /')")
            print("  ‚ùå Unsafe code not detected!")
            return False
        except Exception:
            print("  ‚úÖ Unsafe code detection working")

        # Test safe execution
        result = await sandbox.execute("x = 2 + 2\nprint(f'Result: {x}')")
        assert result["success"]
        assert "Result: 4" in result["stdout"]
        print("  ‚úÖ Safe code execution working")

        # Test Python executor tool
        executor = PythonExecutorTool()
        tool_result = await executor.execute(code="print('Hello, ReasonIt!')")
        assert tool_result.success
        assert "Hello, ReasonIt!" in tool_result.output_data["stdout"]
        print("  ‚úÖ Python executor tool working")

        print("  ‚úÖ Python executor working correctly")

    except Exception as e:
        print(f"  ‚ùå Python executor test failed: {e}")
        return False

    return True


async def test_search_tool():
    """Test search tool (with mocked responses)."""
    print("\nüß™ Testing Search Tool...")

    try:
        from tools.search_tool import SearchProcessor, SearchResult

        # Test search result creation
        result = SearchResult(
            title="Test Article",
            url="https://example.com",
            snippet="This is a test snippet about testing",
            relevance_score=0.8
        )
        assert result.title == "Test Article"
        print("  ‚úÖ SearchResult model working")

        # Test search processor
        processor = SearchProcessor()

        # Create mock results
        mock_results = [
            SearchResult("Python Testing Guide", "https://example.com/1", "Learn how to test Python code"),
            SearchResult("Testing Best Practices", "https://example.com/2", "Best practices for testing"),
            SearchResult("Unrelated Article", "https://example.com/3", "This is about cooking")
        ]

        # Test relevance calculation and filtering
        filtered_results = processor.filter_and_rank("Python testing", mock_results)
        assert len(filtered_results) >= 1
        assert filtered_results[0].relevance_score > 0
        print("  ‚úÖ Search result filtering working")

        # Test summarization
        summary = processor.summarize_results(filtered_results)
        assert "relevant results" in summary.lower()
        print("  ‚úÖ Search result summarization working")

        print("  ‚úÖ Search tool components working correctly")

    except Exception as e:
        print(f"  ‚ùå Search tool test failed: {e}")
        return False

    return True


async def test_verifier_tool():
    """Test verification tool."""
    print("\nüß™ Testing Verifier Tool...")

    try:
        from tools.verifier import AnswerValidator, VerifierTool

        # Test answer validator
        validator = AnswerValidator()

        # Test mathematical validation
        math_result = await validator.validate(
            answer="4",
            validation_type="mathematical",
            criteria={"expected_value": 4, "tolerance": 0.001}
        )
        assert math_result["is_valid"]
        assert math_result["confidence"] > 0.9
        print("  ‚úÖ Mathematical validation working")

        # Test logical validation
        logical_result = await validator.validate(
            answer="true",
            validation_type="logical",
            criteria={"expected_value": True}
        )
        assert logical_result["is_valid"]
        print("  ‚úÖ Logical validation working")

        # Test constraint validation
        constraint_result = await validator.validate(
            answer="hello world",
            validation_type="constraint",
            criteria={
                "constraints": [
                    {"type": "contains", "value": "hello"},
                    {"type": "length", "value": 11}
                ]
            }
        )
        assert constraint_result["is_valid"]
        print("  ‚úÖ Constraint validation working")

        # Test verifier tool
        verifier = VerifierTool()
        tool_result = await verifier.execute(
            answer="42",
            validation_type="numerical",
            criteria={"must_be_positive": True}
        )
        assert tool_result.success
        assert tool_result.output_data["is_valid"]
        print("  ‚úÖ Verifier tool execution working")

        print("  ‚úÖ Verifier tool working correctly")

    except Exception as e:
        print(f"  ‚ùå Verifier tool test failed: {e}")
        return False

    return True


async def test_base_agent_framework():
    """Test base agent framework."""
    print("\nüß™ Testing Base Agent Framework...")

    try:
        from agents import AgentDependencies
        from models import SystemConfiguration

        # Note: We can't fully test the agent without API keys,
        # but we can test the framework components

        # Test agent dependencies
        deps = AgentDependencies(
            session_id="test-session",
            enable_tools=True,
            confidence_threshold=0.8
        )
        assert deps.session_id == "test-session"
        assert deps.enable_tools is True
        print("  ‚úÖ AgentDependencies working")

        # Test system configuration
        config = SystemConfiguration(
            primary_model="gpt-4o-mini",
            max_daily_cost=10.0,
            confidence_threshold=0.7
        )
        assert config.primary_model == "gpt-4o-mini"
        assert config.max_daily_cost == 10.0
        print("  ‚úÖ SystemConfiguration working")

        print("  ‚úÖ Base agent framework components working")

    except Exception as e:
        print(f"  ‚ùå Base agent framework test failed: {e}")
        return False

    return True


async def test_prompt_templates():
    """Test prompt template system."""
    print("\nüß™ Testing Prompt Templates...")

    try:
        from context import PromptTemplates, build_cot_prompt, build_tot_prompt
        from models import ReasoningStrategy

        # Test system prompt retrieval
        cot_prompt = PromptTemplates.get_system_prompt(ReasoningStrategy.CHAIN_OF_THOUGHT)
        assert "step-by-step" in cot_prompt.lower()
        print("  ‚úÖ System prompt retrieval working")

        # Test reasoning template
        reasoning_template = PromptTemplates.get_reasoning_template(ReasoningStrategy.TREE_OF_THOUGHTS)
        assert "approach" in reasoning_template.lower()
        print("  ‚úÖ Reasoning template retrieval working")

        # Test template formatting
        formatted = PromptTemplates.format_template(
            "Hello {name}, welcome to {system}",
            name="User",
            system="ReasonIt"
        )
        assert formatted == "Hello User, welcome to ReasonIt"
        print("  ‚úÖ Template formatting working")

        # Test CoT prompt builder
        cot_prompt = build_cot_prompt("What is 2 + 2?")
        assert "step by step" in cot_prompt
        print("  ‚úÖ CoT prompt builder working")

        # Test ToT prompt builder
        tot_prompt = build_tot_prompt("Solve this puzzle", num_approaches=3)
        assert "3 different approaches" in tot_prompt
        print("  ‚úÖ ToT prompt builder working")

        print("  ‚úÖ Prompt templates working correctly")

    except Exception as e:
        print(f"  ‚ùå Prompt templates test failed: {e}")
        return False

    return True


async def run_integration_test():
    """Run a comprehensive integration test."""
    print("\nüß™ Running Integration Test...")

    try:
        from context import ContextGenerator
        from models import ContextVariant, ReasoningStrategy
        from tools.calculator import SafeMathEvaluator
        from tools.verifier import AnswerValidator

        # Simulate a complete reasoning workflow
        print("  üîÑ Simulating complete reasoning workflow...")

        # 1. Generate context for a math problem
        generator = ContextGenerator()
        original_query = "If x + 5 = 12, what is x?"

        enriched_context = await generator.generate_context(
            original_query,
            ContextVariant.ENRICHED,
            ReasoningStrategy.CHAIN_OF_THOUGHT
        )
        print("  ‚úÖ Context generation completed")

        # 2. Solve the problem using calculator
        evaluator = SafeMathEvaluator()
        # The answer should be x = 7
        answer = evaluator.evaluate("12 - 5")
        print(f"  ‚úÖ Mathematical solution: x = {answer}")

        # 3. Verify the answer
        validator = AnswerValidator()
        verification = await validator.validate(
            answer=str(answer),
            validation_type="mathematical",
            criteria={"expected_value": 7}
        )
        print(f"  ‚úÖ Answer verification: {verification['is_valid']} (confidence: {verification['confidence']:.2f})")

        # 4. Test context cost estimation
        cost_impact = generator.estimate_cost_impact(original_query, ContextVariant.ENRICHED)
        print(f"  ‚úÖ Cost estimation: {cost_impact['token_multiplier']:.1f}x tokens")

        assert verification["is_valid"]
        assert answer == 7

        print("  ‚úÖ Integration test completed successfully")

    except Exception as e:
        print(f"  ‚ùå Integration test failed: {e}")
        return False

    return True


async def main():
    """Run all tests."""
    print("üöÄ Starting ReasonIt Foundation Test Suite")
    print("=" * 60)

    test_results = []

    # Run all test categories
    test_functions = [
        ("Models and Types", test_models_and_types),
        ("Context Generation", test_context_generation),
        ("Tool Framework", test_tool_framework),
        ("Calculator Tool", test_calculator_tool),
        ("Python Executor", test_python_executor),
        ("Search Tool", test_search_tool),
        ("Verifier Tool", test_verifier_tool),
        ("Base Agent Framework", test_base_agent_framework),
        ("Prompt Templates", test_prompt_templates),
        ("Integration Test", run_integration_test),
    ]

    for test_name, test_func in test_functions:
        try:
            result = await test_func()
            test_results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            test_results.append((test_name, False))

    # Print final results
    print("\n" + "=" * 60)
    print("üìä TEST RESULTS SUMMARY")
    print("=" * 60)

    passed = 0
    failed = 0

    for test_name, result in test_results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status:10} {test_name}")
        if result:
            passed += 1
        else:
            failed += 1

    print("=" * 60)
    print(f"üìä TOTAL: {passed + failed} tests, {passed} passed, {failed} failed")

    if failed == 0:
        print("üéâ All tests passed! ReasonIt foundation is working correctly.")
        return True
    else:
        print(f"‚ö†Ô∏è  {failed} test(s) failed. Please review the output above.")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
